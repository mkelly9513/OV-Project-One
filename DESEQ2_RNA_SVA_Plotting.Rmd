---
title: "RNA-seq KO Analysis"
author: "Michael Kelly"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
---

# CRISPR KO Analysis Workflow; SE 60 

# RNA Data Pre-Processing (Unix Environment)
First the raw data was combined into into a single matrix using UNIX tools. 
```{}
paste OV3_WT-2-57925410-201215_NS500276_0197_AH7KV7BGXH_TCTCTACT-CGCGGTTC_S11.read_counts.txt OV3_WT-3-57925410-201215_NS500276_0197_AH7KV7BGXH_GACCTGAA-TTGGTGAG_S10.read_counts.txt OV3_SE60-12-57925408-201215_NS500276_0197_AH7KV7BGXH_ATCCACTG-AGGTGCGT_S5.read_counts.txt OV3_SE60-54-57925409-201215_NS500276_0197_AH7KV7BGXH_CAAGCTAG-ACATAGCG_S7.read_counts.txt > outputfile.txt
```

This makes a file where we have the entrez gene id and symbol for every sample so I needed to cut it down 

To keep entrez and symbol with counts we cut certain columns; I keep the entrez id, the symbol, then the counts for WT1,2 and SE60KO 1,2
```{}
cut -f 1,2,3,6,9,12 outputfile.txt > OV3_WT_SE60_Read_Counts_w_Entrez.txt
```

I next remove the entrez id as the gene symbol is okay for counting and DESEQ2

```{}
cut -f 2,3,4,5,6 OV3_WT_SE60_Read_Counts_w_Entrez.txt > OV3_WT_SE60_Symbol_Read_Counts.txt
```
Then I remove any non-unique lines and sort the file 
```{}
sort -u -k1,1 OV3_WT_SE60_Symbol_Read_Counts.txt > Unique_WT_SE60_Symbol_counts.txt
```

Lastly I use a text editor like vi to enter the output file and remove the first four lines (the counting statistics from htseq) and add column names giving me my input file

```{}
head Unique_WT_SE60_Symbol_counts.txt 
Symbol	WT1	WT2	SE60KO1	SE60KO2
A1BG	12	13	42	66
A1BG-AS1	159	144	170	142
A1CF	0	0	0	0
A2M	2	3	5	8
A2M-AS1	10	7	18	3
A2ML1	3	4	2	5
A2ML1-AS1	1	0	0	2
A2ML1-AS2	0	0	0	0
A2MP1	46	40	75	60

```

# Prepare Environment 
```{r setup, include=FALSE}
library(DESeq2)
library(vsn)
library(ggplot2)
library(statmod)
library(pheatmap)
library(amap)
library(RColorBrewer)

```

# Import Data and begin processing 

1) First I want to import our data and look at it's structure to ensure I understand its format and that it contains all the information I want

```{r}
x <- read.delim("Unique_WT_SE60_Symbol_counts.txt",row.names="Symbol")
x[1:10,]
```

I next want to remove all genes with no counts across my samples as they will cause issues with normalization and variance adjustments

```{r}
x.sub<- x[rowSums(x) >1 ,]
dim(x)
dim(x.sub)
```

As can be seen we remove about 20k features that had 0 counts 

Next I want to create a DESEQ data object from this matrix; this will be done by creating group assignments (2 WT 2 KO)

```{r}
group <- factor(c('CNTRL','CNTRL','KO','KO'))
design <- model.matrix(~group)
des<-data.frame(design)
#I like to re-name columns to things that make more sense to me; this will give me a matrix with a sample column and an identity column with 0 for alpha and 1 for beta 
des$X.Intercept. <- NULL
des$sample<- as.factor(colnames(x.sub))
des$designation<- as.factor(des$groupKO)
des$groupKO<- NULL
des

```
 I can now use this as to build a condition aware DESEQ dataset 
```{r}
dex<-DESeqDataSetFromMatrix(x.sub,design = ~designation, colData = des)
colnames(dex)<-des$sample
dex
design(dex)= ~ 1+designation 
```

# Data Exploration: Raw Counts
I next want to explore what this data looks like e.g. how similar are the "replicates", are there outlier effects, whats the high dimension structure, etc.

```{r}
#within WT cells
plot(assay(dex)[,1],assay(dex)[,2])


#now looking at correlations of raw data 
cor(assay(dex)[,1],assay(dex)[,2])


```

```{r}
#within KO cells
plot(assay(dex)[,3],assay(dex)[,4])


#now looking at correlations 
cor(assay(dex)[,3],assay(dex)[,4])


```

# Takeaway pre-normalized Data
We can clearly see that there is a some inter-group variation in the raw-count space; this is not suprising however as we have not adjusted for sequencing depth or performed any kind of scaling. This data will be normalized through the DESEQ2 differential expression analysis pipeline; however, for investigation and visualization purposes the VST transformation is a good surrogate for what goes on under the hood in DESEQ2.

```{r}
colSums(assay(dex))
```

# Data Exploration post VST Normalization

I can now look at if normalizing via a method such as VST can improve some of these issues. In theory VST should adjust for outliers, sequencing depth, and differences in variance structure drastically improving the correlations between our samples and making comparisons much cleaner. 

```{r}
vsd <- vst(dex)

#WT Cells
plot(assay(vsd)[,1],assay(vsd)[,2])

cor(assay(vsd)[,1],assay(vsd)[,2])


```

```{r}
#KO Cells
plot(assay(vsd)[,3],assay(vsd)[,4])

cor(assay(vsd)[,3],assay(vsd)[,4])


```

From this analysis we can determine that these samples are closer to their "replicates" now than they were prieviously; while the wild-type is almost perfectly correlated there appears to be some slight differences in the two independant KO clones. This is expected however as these are biologically independant.  

One way to investigate what is happening in the data is through looking at a low dimension representation of the data such as PCA. PCA will allow us to view the overall data structure and determine if there is sample seperation on biologcal or technical levels, ideally we want PC1 to seperate WT and KO samples as that will be the largest source of varience in the data. 




```{r}
pcaData <-DESeq2::plotPCA(vsd, ntop=20000, intgroup = c("sample","designation") ,returnData = TRUE)

percentVar <- round(100 * attr(pcaData, "percentVar")) 
ggplot(pcaData, aes(x = PC1, y = PC2, color = factor(sample), shape = factor(designation))) + 
geom_point(size =3, aes(fill=factor(sample))) + 
geom_point(size =3) + 
scale_shape_manual(values=c(21,22)) + 
scale_alpha_manual(values=c("F"=0, "M"=1)) + 
xlab(paste0("PC1: ", percentVar[1], "% variance")) + 
ylab(paste0("PC2: ", percentVar[2], "% variance")) + 
ggtitle("PCA of all genes, 0 (circles) being WT") 
```

We can also look at the top 3000 most variable features. 

# Heatmaps
```{r}
vsd<-vst(dex)
logcts<- assay(vsd)

rv <- rowVars(logcts)
o <- order(rv, decreasing=TRUE)[1:3000]
qlfvals<- assay(vsd)

dists <- Dist(t(qlfvals[o,]), method = 'pearson')
cols <- colorRampPalette(c("blue","steelblue","white",'salmon',"red"))(255)
distmat <- as.matrix(dists)
df <- data.frame(condition=colnames(qlfvals[o,]),
                 row.names=colnames(distmat))
pheatmap(qlfvals[o,],scale = 'row',
         color = cols,
         #clustering_distance_rows= ,
         clustering_distance_cols=dists,
         annotation_col=df,
         show_rownames=FALSE, show_colnames=TRUE)


```

#Takeaway VST Normalized Data
From this low-dimension view of the data it is clear that while PC1 seperates our WT condition from the KO condition there is some slight variation in the KO analysis group. This suggests that there may be a small technical or batch effect present in the data that may interfere with downstream analysis. I can investigate this by hunting for latent variables using SVA (part of the svn package) and including any indentified SV's into our DESEQ2 design matrix informing the model of this unwanted variation.


# Adjustment for Technical Effects Via SVA

As it appears that there may be technical variables at play I want to investigate if we can determine "non-biological" sources of variation present in the data which we can use to improve the DESEQ model. We can do this using a package called SVA (surrogate variable analysis) where we can compare a model that is aware of the biological difference of interest (in our case WT vs KO) vs one that is agnositc to the biological differences in the data. By adjusting for the variation of interest we can ideally identify technical factors that may be excerting unwanted influence on the data and our future differential expression analysis. Based on the PCA plot above I would expect that the primary surrogate variable will seperate the two KO clones and this will be something we may want to address.


```{r}
dex2 <- estimateSizeFactors(dex)
norm.cts <- counts(dex2, normalized=TRUE)
library(sva)
mm <- model.matrix(~ designation, colData(dex2))
mm0 <- model.matrix(~ 1, colData(dex2))
norm.cts <- norm.cts[rowSums(norm.cts) > 0,]

#at this point we determine the surrogate variables by using n.sv, in this example we are looking to see if there is at least 1 significant SV
fit <- svaseq(norm.cts, mod=mm, mod0=mm0, n.sv=1)

#we can then look at the data in low dimension space to see if the significant SV seems important 
plot(fit$sv,pch=19,col=dex2$designation)


```
If we focus on the Y axis, which is the variation from our primary surrogate variable SV1 (or in this case the only surrogate variable) it is clear that it seperates the two KO clones and this variation is something we may want to inform the DESEQ model of. 

To start I will just inform the DESEQ model of the SV1 variation to see how that effects results and ignore SV2, as adjusting for too much variation may be harmful.

I can do this by adding in the SV1 information to the DESEQ object, and including it as a term in the design matrix 
```{r}
dex2$SV1 = fit$sv[,1]
design(dex2)= ~ SV1 + designation 

```

I can then run DESEQ2 with this updated object and design matrix and determine if there are differences between the results with the inclusion of this SV as a factor. 

# Differential Expression Analysis with Inclusion of Information about Technical Variation
```{r}
desdex2<-DESeq2::DESeq(dex2)
res <- results(desdex2)
res <- res[order(res$padj),]
resdown<-res[na.omit(res$log2FoldChange < -2),]
resSig <- subset(resdown, padj < 0.005)
summary(resSig)
```




# Differential Expression Analysis without Adjusting for Batch/Technical Variables 
Now we can run DESEQ2 Differential Expression Analysis 
```{r}
desdex<-DESeq2::DESeq(dex)
```


Then look at the results 

```{r}
res <- results(desdex)
res <- res[order(res$padj),]
resdown<-res[na.omit(res$log2FoldChange < -2),]
resSignorm <- subset(resdown, padj < 0.005)
summary(resSignorm)
```


# Comparison

While they seem to give very similar results the model with SV1 included performs slightly better in terms of DEGS. I can look to make sure it is capturing most of the base model's results by comparing the genes in each group. 
```{r}
sum(rownames(resSig) %in% rownames(resSignorm))
```

In summary 241 of the 245 genes in the base model are captured by the SV inclusion model and I get another 50 genes unique to this model. As such, we felt that either model would suffice and be fair to use so we continued with the SV inclusion model. 




